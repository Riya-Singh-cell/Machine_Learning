{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b5d3ee0",
   "metadata": {},
   "source": [
    "# üéØ Feature Engineering ‚Äî Part A: Individual Concepts (Colab-Ready)\n",
    "\n",
    "**Updated:** 2025-08-22\n",
    "\n",
    "This notebook is designed for **first-time learners**. You will practice each feature engineering step **individually** (no pipelines yet), so you can clearly see *what each step does* and *why it matters*.\n",
    "\n",
    "**What you'll practice:**\n",
    "- Dataset loading & quick audit\n",
    "- Handling missing values (drop, impute)\n",
    "- Scaling & normalization (standardization, min-max, per-row normalization)\n",
    "- Encoding categorical variables (ordinal vs one-hot)\n",
    "- Feature transformations (log, power, polynomial)\n",
    "- Simple dimensionality reduction (PCA) for visualization\n",
    "- Short exercises after each section\n",
    "\n",
    "> Use this Part A first. After you are comfortable, move to **Part B (Pipelines)** to automate and combine steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04499a3f",
   "metadata": {},
   "source": [
    "## 0) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cd58db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running in Google Colab, you can install optional packages here:\n",
    "!pip install -q statsmodels==0.14.2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer, OrdinalEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures, PowerTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de06ce7",
   "metadata": {},
   "source": [
    "## 1) Dataset Setup & Quick Audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fda59e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Load Titanic from a stable GitHub mirror (recommended for first run)\n",
    "URL = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "df = pd.read_csv(URL)\n",
    "print(\"Shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6588cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: Upload your own CSV (uncomment to use in Colab)\n",
    "# from google.colab import files\n",
    "# up = files.upload()  # pick file\n",
    "# import io\n",
    "# df = pd.read_csv(io.BytesIO(up[list(up.keys())[0]]))\n",
    "# print(\"Shape:\", df.shape)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1020b508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick audit\n",
    "print(\"\\nInfo:\")\n",
    "df.info()\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isna().sum().sort_values(ascending=False))\n",
    "print(\"\\nNumeric describe:\")\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84781f19",
   "metadata": {},
   "source": [
    "## 2) Handling Missing Values (Individually)\n",
    "\n",
    "**Goal:** Learn when to **drop** vs **impute**.\n",
    "\n",
    "**Common choices**\n",
    "- Numeric: mean/median\n",
    "- Categorical: most frequent\n",
    "\n",
    "We'll practice on Titanic columns like `Age`, `Embarked`, and `Cabin`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865e4694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View null counts\n",
    "df.isna().sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ae03bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 DROP example (use cautiously)\n",
    "df_drop_rows = df.dropna(subset=['Age', 'Embarked'])  # drop rows where these are null\n",
    "print(\"Original:\", df.shape, \"After drop:\", df_drop_rows.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1613fc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 SIMPLE IMPUTE example\n",
    "df_imp = df.copy()\n",
    "# Numeric (Age): median\n",
    "df_imp['Age'] = df_imp['Age'].fillna(df_imp['Age'].median())\n",
    "# Categorical (Embarked): most frequent\n",
    "df_imp['Embarked'] = df_imp['Embarked'].fillna(df_imp['Embarked'].mode()[0])\n",
    "\n",
    "# 'Cabin' is very sparse; we can fill with \"Unknown\"\n",
    "df_imp['Cabin'] = df_imp['Cabin'].fillna('Unknown')\n",
    "\n",
    "df_imp.isna().sum().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346a57b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 KNN Imputation (numeric only demonstration)\n",
    "num_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "knn_df = df[num_cols].copy()\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "knn_imputed = imputer.fit_transform(knn_df)\n",
    "knn_imputed_df = pd.DataFrame(knn_imputed, columns=num_cols)\n",
    "knn_imputed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9902aff5",
   "metadata": {},
   "source": [
    "**üìù Exercise 2**\n",
    "1) Compare **mean vs median** imputation for `Age`. Which preserves the original distribution better?  \n",
    "2) For `Embarked`, try filling with a new category (`'Unknown'`) vs mode. What changes in `value_counts()`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5b4f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'].dropna().hist(bins=30)\n",
    "plt.title(\"Original Age Distribution\")\n",
    "plt.show()\n",
    "df_mean = df.copy()\n",
    "df_mean['Age'] = df_mean['Age'].fillna(df_mean['Age'].mean())\n",
    "df_median = df.copy()\n",
    "df_median['Age'] = df_median['Age'].fillna(df_median['Age'].median())\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,4))\n",
    "df_mean['Age'].hist(bins=30, ax=ax[0])\n",
    "ax[0].set_title(\"Mean Imputation\")\n",
    "df_median['Age'].hist(bins=30, ax=ax[1])\n",
    "ax[1].set_title(\"Median Imputation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d255d0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mode = df.copy()\n",
    "df_mode['Embarked'] = df_mode['Embarked'].fillna(df_mode['Embarked'].mode()[0])\n",
    "df_unknown = df.copy()\n",
    "df_unknown['Embarked'] = df_unknown['Embarked'].fillna('Unknown')\n",
    "print(\"Mode fill:\\n\", df_mode['Embarked'].value_counts())\n",
    "print(\"\\nUnknown fill:\\n\", df_unknown['Embarked'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acba27c3",
   "metadata": {},
   "source": [
    "## 3) Scaling & Normalization (Individually)\n",
    "\n",
    "- **Standardization**: z = (x - mean)/std (good for many ML models)\n",
    "- **MinMax scaling**: maps to [0,1] (useful when features have different units)\n",
    "- **Per-row Normalization**: scales each *row vector* to unit norm (useful for text-like frequency vectors)\n",
    "\n",
    "We'll demonstrate on `Fare` and `Age`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5f5268",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10,4))\n",
    "axes[0].hist(df_imp['Age'].dropna(), bins=30)\n",
    "axes[0].set_title('Age - Raw')\n",
    "axes[1].hist(df_imp['Fare'].dropna(), bins=30)\n",
    "axes[1].set_title('Fare - Raw')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2196adb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_std = StandardScaler()\n",
    "sc_mm  = MinMaxScaler()\n",
    "\n",
    "age_std = sc_std.fit_transform(df_imp[['Age']])\n",
    "fare_mm = sc_mm.fit_transform(df_imp[['Fare']])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10,4))\n",
    "axes[0].hist(age_std.flatten(), bins=30)\n",
    "axes[0].set_title('Age - Standardized')\n",
    "axes[1].hist(fare_mm.flatten(), bins=30)\n",
    "axes[1].set_title('Fare - MinMax [0,1]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d456b1",
   "metadata": {},
   "source": [
    "**üìù Exercise 3**\n",
    "1) Standardize `Fare` and plot the histogram.  \n",
    "2) Apply **Normalizer** on `[Age, Fare]` rows and check the first 5 normalized vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466b4f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "fare_std = scaler.fit_transform(df_imp[['Fare']])\n",
    "fare_std_df = pd.DataFrame(fare_std, columns=['Fare_std'])\n",
    "plt.figure(figsize=(6,4))\n",
    "fare_std_df['Fare_std'].hist(bins=30, edgecolor='black')\n",
    "plt.title(\"Standardized Fare (z-score)\")\n",
    "plt.xlabel(\"Standardized Fare\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4971116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "X = df_imp[['Age', 'Fare']].copy()\n",
    "norm = Normalizer()\n",
    "X_norm = norm.fit_transform(X)\n",
    "X_norm_df = pd.DataFrame(X_norm, columns=['Age_norm', 'Fare_norm'])\n",
    "print(\"First 5 normalized vectors:\")\n",
    "print(X_norm_df.head())\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf2e146",
   "metadata": {},
   "source": [
    "## 4) Encoding Categorical Variables (Individually)\n",
    "\n",
    "- **Ordinal/Label encoding**: map categories to integers (assumes order or used with tree models).  \n",
    "- **One-Hot encoding**: binary column per category (no order assumption).\n",
    "\n",
    "We'll use `Sex` and `Embarked` as examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a961c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Ordinal encoding demo (note: no real order in Sex/Embarked; this is just to illustrate)\n",
    "enc = OrdinalEncoder()\n",
    "ord_demo = df_imp[['Sex','Embarked']].copy()\n",
    "ord_vals = enc.fit_transform(ord_demo)\n",
    "pd.DataFrame(ord_vals, columns=['Sex_ord','Embarked_ord']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb67c353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 One-Hot encoding demo with pandas\n",
    "ohe_embarked = pd.get_dummies(df_imp['Embarked'], prefix='Embarked')\n",
    "ohe_sex = pd.get_dummies(df_imp['Sex'], prefix='Sex')\n",
    "encoded_df = pd.concat([df_imp[['Survived','Age','Fare']], ohe_sex, ohe_embarked], axis=1)\n",
    "encoded_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be0cd1f",
   "metadata": {},
   "source": [
    "**üìù Exercise 4**\n",
    "1) Compare the **number of features** produced by ordinal vs one-hot for `Embarked`.  \n",
    "2) Why might one-hot be safer for linear models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d80b4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embarked\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "enc = OrdinalEncoder()\n",
    "ord_vals = enc.fit_transform(df_imp[['Embarked']])\n",
    "ord_embarked = pd.DataFrame(ord_vals, columns=['Embarked_ord'])\n",
    "print(\"Ordinal encoding:\\n\", ord_embarked.head())\n",
    "print(\"Number of features (Embarked, ordinal):\", ord_embarked.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9d1ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_embarked = pd.get_dummies(df_imp['Embarked'], prefix='Embarked')\n",
    "print(\"One-hot encoding shape:\", ohe_embarked.shape)\n",
    "print(ohe_embarked.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c453d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c161dfa",
   "metadata": {},
   "source": [
    "## 5) Feature Transformation (Individually)\n",
    "\n",
    "- **Log transform**: t = log1p(x) for right-skewed positive data (e.g., Fare).\n",
    "- **Power transform**: Yeo-Johnson can handle zero/negative values; stabilizes variance.\n",
    "- **Polynomial features**: create interactions/quadratics for simple non-linear modeling.\n",
    "\n",
    "We'll use `Fare` and `Age`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5012b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Log transform on Fare (positive values)\n",
    "fare_raw = df_imp['Fare'].dropna().values.reshape(-1,1)\n",
    "fare_log = np.log1p(fare_raw)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10,4))\n",
    "axes[0].hist(fare_raw.flatten(), bins=30)\n",
    "axes[0].set_title('Fare - Raw')\n",
    "axes[1].hist(fare_log.flatten(), bins=30)\n",
    "axes[1].set_title('Fare - log1p')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c34a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Power transform (Yeo-Johnson) on [Age, Fare]\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "af = df_imp[['Age','Fare']].dropna()\n",
    "af_pt = pt.fit_transform(af)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10,4))\n",
    "axes[0].hist(af['Age'].values, bins=30)\n",
    "axes[0].set_title('Age - Raw')\n",
    "axes[1].hist(af_pt[:,0], bins=30)\n",
    "axes[1].set_title('Age - Yeo-Johnson')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d9c780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 Polynomial features on [Age, Fare] (degree=2)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "af_poly = poly.fit_transform(af[['Age','Fare']])\n",
    "print(\"Original shape:\", af[['Age','Fare']].shape, \" -> With poly:\", af_poly.shape)\n",
    "poly.get_feature_names_out(['Age','Fare'])[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f15002",
   "metadata": {},
   "source": [
    "**üìù Exercise 5**\n",
    "1) Identify one numeric column that is **skewed**. Try both **log** and **power** transforms and compare histograms.  \n",
    "2) With `PolynomialFeatures(2)`, which new terms are created from `Age` and `Fare`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a51474e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew\n",
    "fare_raw = df_imp['Fare'].dropna().values.reshape(-1,1)\n",
    "print(\"Skewness (raw Fare):\", skew(fare_raw.flatten()))\n",
    "fare_log = np.log1p(fare_raw)\n",
    "print(\"Skewness (log1p Fare):\", skew(fare_log.flatten()))\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "fare_pt = pt.fit_transform(fare_raw)\n",
    "print(\"Skewness (Yeo-Johnson Fare):\", skew(fare_pt.flatten()))\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15,4))\n",
    "axes[0].hist(fare_raw.flatten(), bins=30, edgecolor='black')\n",
    "axes[0].set_title(\"Fare - Raw\")\n",
    "axes[1].hist(fare_log.flatten(), bins=30, edgecolor='black')\n",
    "axes[1].set_title(\"Fare - Log1p\")\n",
    "axes[2].hist(fare_pt.flatten(), bins=30, edgecolor='black')\n",
    "axes[2].set_title(\"Fare - Yeo-Johnson\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b550f5f",
   "metadata": {},
   "source": [
    "## 6) Simple Dimensionality Reduction (PCA) ‚Äî Visualization Only\n",
    "\n",
    "We will apply PCA to **numeric** features to reduce to 2D and make a scatter plot colored by `Survived` (if present).\n",
    "\n",
    "> Note: This is for **intuition/visualization** only in Part A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888f66b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a numeric-only frame (drop NA for simplicity here)\n",
    "num_only = df_imp.select_dtypes(include=['number']).dropna()\n",
    "y = df_imp.loc[num_only.index, 'Survived'] if 'Survived' in df_imp.columns else None\n",
    "\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "Z = pca.fit_transform(num_only.values)\n",
    "\n",
    "print(\"Explained variance ratios:\", pca.explained_variance_ratio_)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6,5))\n",
    "if y is not None:\n",
    "    # Color by Survived (0/1). Using a simple split to avoid specifying colors.\n",
    "    idx0 = (y.values == 0)\n",
    "    idx1 = (y.values == 1)\n",
    "    plt.scatter(Z[idx0,0], Z[idx0,1], s=10, label='Survived=0')\n",
    "    plt.scatter(Z[idx1,0], Z[idx1,1], s=10, label='Survived=1')\n",
    "    plt.legend()\n",
    "else:\n",
    "    plt.scatter(Z[:,0], Z[:,1], s=10)\n",
    "plt.xlabel('PC1'); plt.ylabel('PC2'); plt.title('PCA (numeric only)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade92af5",
   "metadata": {},
   "source": [
    "**üìù Exercise 6**\n",
    "1) Which **two numeric columns** contribute the most variance before PCA (use `df.var()`)?  \n",
    "2) Try PCA with `n_components=3` and print the cumulative explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da62fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vars = df_imp.select_dtypes(include=['number']).var().sort_values(ascending=False)\n",
    "print(num_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f685237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "num_only = df_imp.select_dtypes(include=['number']).dropna()\n",
    "pca3 = PCA(n_components=3, random_state=42)\n",
    "Z3 = pca3.fit_transform(num_only.values)\n",
    "print(\"Explained variance ratios:\", pca3.explained_variance_ratio_)\n",
    "print(\"Cumulative explained variance:\", np.cumsum(pca3.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d52dd9c",
   "metadata": {},
   "source": [
    "## 7) Consolidated Practice (No Pipelines Yet)\n",
    "\n",
    "Using the operations you've learned, perform a **clean preprocessing** (manually):\n",
    "1) Impute: `Age` (median), `Embarked` (mode), `Cabin` ('Unknown').  \n",
    "2) Scale: standardize `Age` and min-max scale `Fare`.  \n",
    "3) Encode: one-hot `Sex` and `Embarked`.  \n",
    "4) Transform: log1p `Fare`.  \n",
    "5) (Optional) PCA on numeric subset for 2D visualization.\n",
    "\n",
    "Then, answer:\n",
    "- Which step **changed the data distribution** the most?\n",
    "- Which encoding produced **more features**, ordinal or one-hot? Why?\n",
    "- If you trained a simple logistic regression on your manually processed features, what **accuracy** do you get on a 75/25 split? (Optional challenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb42e6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "URL = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "df = pd.read_csv(URL)\n",
    "df['Age'] = df['Age'].fillna(df['Age'].median())\n",
    "df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n",
    "df['Cabin'] = df['Cabin'].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5bf2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "age = StandardScaler()\n",
    "fare = MinMaxScaler()\n",
    "df['Age_scaled'] = age.fit_transform(df[['Age']])\n",
    "df['Fare_scaled'] = fare.fit_transform(df[['Fare']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd6f3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.get_dummies(df, columns=['Sex', 'Embarked'], drop_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141e458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Fare_log'] = np.log1p(df['Fare'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9af9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "numeric_subset = df[['Age_scaled', 'Fare_scaled']]\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(numeric_subset)\n",
    "plt.scatter(pca_result[:,0], pca_result[:,1])\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('PCA 2D Projection')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4251ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = df[['Age_scaled', 'Fare_scaled', 'Fare_log',\n",
    "        'Sex_female', 'Sex_male',\n",
    "        'Embarked_C', 'Embarked_Q', 'Embarked_S']]\n",
    "y = df['Survived']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "log_reg = LogisticRegression(max_iter=500)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred = log_reg.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7413b3",
   "metadata": {},
   "source": [
    "## ‚úÖ What You Should Take Away from Part A\n",
    "\n",
    "- Each step (imputation, scaling, encoding, transforms) has a **clear purpose** and **visible effect**.  \n",
    "- You can now apply them **manually** and reason about their impact.  \n",
    "- Next: move to **Part B (Pipelines)** to **combine & automate** these steps safely (avoid leakage, enable cross-validation, and reproducibility)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
